name: AI - Train then Verify (Devnet Skeleton)

on:
  workflow_dispatch: {}

jobs:
  train-then-verify-devnet:
    name: Train tiny devnet model + verify hash compatibility
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install minimal trainer deps (tiny run)
        run: |
          python -m pip install --upgrade pip
          pip install "numpy==1.26.4" "pandas==2.2.2" "scikit-learn==1.5.2" "lightgbm==4.3.0" "blake3==0.4.1"

      - name: Generate tiny devnet-style dataset
        run: |
          python - <<'PY'
          import csv
          import os
          from pathlib import Path
          SCALE = 1_000_000
          out_dir = Path("ai_assets/datasets/devnet")
          out_dir.mkdir(parents=True, exist_ok=True)
          out = out_dir / "devnet_dataset_20990101T000000Z.csv"
          cols = [
              "timestamp_utc","round_id","validator_id",
              "uptime_ratio_7d","validated_blocks_7d","missed_blocks_7d","avg_latency_ms",
              "slashing_events_90d","stake_normalized","peer_reports_quality","fairness_score_scaled",
          ]
          with out.open("w", newline="", encoding="utf-8") as f:
              w = csv.writer(f)
              w.writerow(cols)
              # 40 rows, 5 validators x 8 rounds
              for r in range(8):
                  for v in range(5):
                      vid = f"{v:064x}"
                      uptime = int((0.90 + 0.01*v) * SCALE)
                      stake = int((0.20 + 0.10*v) * SCALE)
                      peerq = int((0.80 + 0.02*v) * SCALE)
                      latency = int((0.10 + 0.05*v) * SCALE)
                      validated = int((10 + r + v) * SCALE)
                      missed = int((v % 2) * SCALE)
                      slash = v % 3
                      fairness = min(SCALE, max(0, int(0.6*SCALE + 0.2*uptime + 0.1*stake + 0.1*peerq - 0.1*latency)))
                      ts = f"2099-01-01T00:00:{r:02d}Z"
                      w.writerow([ts, r, vid, uptime, validated, missed, latency, slash, stake, peerq, fairness])
          print(f"Wrote {out}")
          PY

      - name: Train (devnet entrypoint) and capture hash
        run: |
          set -euo pipefail
          python ai_training/train_ippan_d_gbdt_devnet.py \
            --data-dir ai_assets/datasets/devnet \
            --output-dir ai_assets/models/devnet \
            --model-id ippan_d_gbdt_devnet_ci_smoke \
            | tee train.log
          grep -E '^model_path=' train.log
          grep -E '^model_hash=' train.log

      - name: Set up Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Verify Rust hash matches trainer hash (canonical JSON)
        run: |
          set -euo pipefail
          MODEL_PATH="$(grep -E '^model_path=' train.log | tail -n 1 | cut -d= -f2-)"
          PY_HASH="$(grep -E '^model_hash=' train.log | tail -n 1 | cut -d= -f2-)"
          RS_HASH="$(cargo run -q -p ippan-ai-core --bin compute_model_hash -- "$MODEL_PATH")"
          echo "PY_HASH=$PY_HASH"
          echo "RS_HASH=$RS_HASH"
          test "$PY_HASH" = "$RS_HASH"

      - name: Determinism smoke: retrain and ensure hash stable
        run: |
          set -euo pipefail
          python ai_training/train_ippan_d_gbdt_devnet.py \
            --data-dir ai_assets/datasets/devnet \
            --output-dir ai_assets/models/devnet \
            --model-id ippan_d_gbdt_devnet_ci_smoke \
            | tee train2.log
          HASH1="$(grep -E '^model_hash=' train.log | tail -n 1 | cut -d= -f2-)"
          HASH2="$(grep -E '^model_hash=' train2.log | tail -n 1 | cut -d= -f2-)"
          echo "HASH1=$HASH1"
          echo "HASH2=$HASH2"
          test "$HASH1" = "$HASH2"


